{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/melody/Documents/Courses 2021 spring/ling_programming/term project',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python38.zip',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8',\n",
       " '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/Users/melody/Documents/Courses 2021 spring/ling_programming/term project/pdf_ocr/lib/python3.8/site-packages',\n",
       " '/Users/melody/Documents/Courses 2021 spring/ling_programming/term project/pdf_ocr/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/Users/melody/.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install other dependencies:\n",
    "- [tesseract](https://github.com/tesseract-ocr/tesseract/releases)\n",
    "    - Mac: brew install tesseract \n",
    "- poppler\n",
    "    - Windoes: download [here](), install, add to PATH\n",
    "    - Mac: brew install poppler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem with importing cv2\n",
    "- check if juyter is installed in virtual env\n",
    "- check if \n",
    "```\n",
    "import sys\n",
    "sys.path\n",
    "```\n",
    "in both jupyter notebook andn in terminal have the same path\n",
    "\n",
    "- if error message is ```ImportError: libGL.so.1: cannot open shared object file: No such file or directory```:  \n",
    "do in terminal:\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt install libgl1-mesa-glx\n",
    "```\n",
    "source: https://github.com/conda-forge/pygridgen-feedstock/issues/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#from pytesser.pytesser import *\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path, convert_from_bytes\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "poppler resources:\n",
    "https://github.com/Belval/pdf2image/issues/101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PpmImagePlugin.PpmImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# convert pdf to images\n",
    "# v useful tutorial: https://www.geeksforgeeks.org/convert-pdf-to-image-using-python/\n",
    "\n",
    "images = convert_from_path('sample-papers/assym.pdf', dpi=100, first_page=1, last_page=1)\n",
    "print(type(images[0]))\n",
    "# for i in range(len(images)):\n",
    "#     images[i].save('page'+ str(i) +'.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].save('page_1.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### do it with gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can even do it with a gui!!\n",
    "# package needed: tkinter\n",
    "\n",
    "# from pdf2image import convert_from_path\n",
    "# from tkinter import *\n",
    "# from tkinter import messagebox\n",
    " \n",
    "# def pdf2img():\n",
    "#     try:\n",
    "#         images = convert_from_path(str(e1.get()))\n",
    "#         for img in images:\n",
    "#             img.save('new_folder\\output.jpg', 'JPEG')\n",
    " \n",
    "#     except  :\n",
    "#         Result = \"NO pdf found\"\n",
    "#         messagebox.showinfo(\"Result\", Result)\n",
    " \n",
    "#     else:\n",
    "#         Result = \"success\"\n",
    "#         messagebox.showinfo(\"Result\", Result)\n",
    " \n",
    "\n",
    "# master = Tk()\n",
    "# Label(master, text=\"File Location\").grid(row=0, sticky=W)\n",
    " \n",
    "# e1 = Entry(master)\n",
    "# e1.grid(row=0, column=1)\n",
    " \n",
    "# b = Button(master, text=\"Convert\", command=pdf2img)\n",
    "# b.grid(row=0, column=2,columnspan=2, rowspan=2,padx=5, pady=5)\n",
    "  \n",
    "# mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "img1 = np.array(Image.open('page_1.jpg'))\n",
    "text = pytesseract.image_to_string(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this no longer works!!\n",
    "import tesserocr\n",
    "with tesserocr.PyTessBaseAPI() as api:\n",
    "    #image = Image.open(io.BytesIO(req_image))\n",
    "    api.SetImage(Image.open('skr.png'))\n",
    "    api.Recognize()  # required to get result from the next line\n",
    "    iterator = api.GetIterator()\n",
    "    print(iterator\n",
    "    #print(iterator.WordFontAttributes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for getting font size:\n",
    "# try this mayvbe?\n",
    "# source: https://stackoverflow.com/questions/58317647/how-to-find-the-font-size-in-python-from-an-image\n",
    "# have a look at this as well: https://levelup.gitconnected.com/how-to-properly-calculate-text-size-in-pil-images-17a2cc6f51fd\n",
    "\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "def find_font_size(text, font, image, target_width_ratio):\n",
    "    tested_font_size = 100\n",
    "    tested_font = ImageFont.truetype(font, tested_font_size)\n",
    "    observed_width, observed_height = get_text_size(text, image, tested_font)\n",
    "    estimated_font_size = tested_font_size / (observed_width / image.width) * target_width_ratio\n",
    "    return round(estimated_font_size)\n",
    "\n",
    "def get_text_size(text, image, font):\n",
    "    im = Image.new('RGB', (image.width, image.height))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    return draw.textsize(text, font)\n",
    "\n",
    "width_ratio = 0.5\n",
    "font_family = \"arial.ttf\"\n",
    "text = \"Hello World\"\n",
    "\n",
    "image = Image.open('pp.png')\n",
    "editable_image = ImageDraw.Draw(image)\n",
    "font_size = find_font_size(text, font_family, image, width_ratio)\n",
    "font = ImageFont.truetype(font_family, font_size)\n",
    "print(f\"Font size found = {font_size} - Target ratio = {width_ratio} - Measured ratio = {get_text_size(text, image, font)[0] / image.width}\")\n",
    "\n",
    "editable_image.text((10, 10), text, font=font)\n",
    "image.save('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract title only by splitting string by the first single line cintaining only \\n\n",
    "# since titles are often the first few lines in a document and are separated from the rest of the test\n",
    "# with a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vy 2021', '', '06912v1 [cs.CL] 14M', '', '5:', '', 'arXiv:210.', '', 'a', '', ' ', '', 'QAConv: Question Answering on Informative', 'Conversations', '', ' ', '', \"Chien-Sheng Wu', Andrea Madotto', Wenhao Liu', Pascale Fung’, Caiming Xiong!\", '‘Salesforce Al Research', '', '+The Hong Kong University of Sci', '', '{wu. jason, venhao.liu, cxiong}@salesforce.com anadotto@connect .ust.hk', '', ' ', '', 'Abstract.', '', '‘This paper introduces QACom{|] a new question answering (QA) dataset that uses', 'conversations as a knowledge source. We focus on informative conversations', 'including business emails, panel discussions, and work channels. Unlike open-', 'domain and task-oriented dialogues, these conversations are usually long, complex,', 'asynchronous, and involve strong domain knowledge. In total, we collect 34,204', 'QA pairs, including span-based, free-form, and unanswerable questions, from', '10,259 selected conversations with both human-written and machine-generated', 'questions. We segment long conversations into chunks, and use a question gen-', 'erator and dialogue summarizer as auxiliary tools to collect multi-hop questions.', '‘The dataset has two testing scenarios, chunk mode and full mode, depending on', 'whether the grounded chunk is provided or retrieved from a large conversational', 'pool. Experimental results show that state-of-the-art QA systems trained on exist-', 'ing QA datasets have limited zero-shot ability and tend to predict our questions', 'as unanswerable, Fine-tuning such systems on our corpus can achieve signifi-', 'cant improvement up to 23.6% and 13.6% in both chunk mode and full mode,', 'respectively', '', ' ', '', ' ', '', ' ']\n"
     ]
    }
   ],
   "source": [
    "splt_txt = text.split('\\n')\n",
    "print(splt_txt[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lightweight, Dynamic Graph Convolutional Networks', 'for AMR-to-Text Generation']\n"
     ]
    }
   ],
   "source": [
    "splt_text = text.split('\\n')\n",
    "\n",
    "title = []\n",
    "for line in splt_text:\n",
    "    if line == '':\n",
    "        title = splt_text[:splt_text.index(line)]\n",
    "        break\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get title:\n",
    "use text fonts with text box -> text block with largest font will be title\n",
    "\n",
    "get paper info:\n",
    "google with title -> get info from google scholar (crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './page_1.jpg'\n",
    "#image = cv2.imread(filename)\n",
    "image = Image.open(filename)\n",
    "\n",
    "# crop image to reduce processing time\n",
    "# Size of the image in pixels (size of orginal image)\n",
    "width, height = image.size\n",
    "  \n",
    "# Setting the points for cropped image\n",
    "# upper left corner is (0, 0)\n",
    "left = 0\n",
    "top = 0\n",
    "right = width\n",
    "bottom = height / 2\n",
    "\n",
    "image = np.array(image.crop((left, top, right, bottom)))\n",
    "res = pytesseract.image_to_data(image, output_type=Output.DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thought: 找最大值-->連續三個都與最大值相差超過 x --> new max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract bounding box \n",
    "# ref: https://fahmisalman.medium.com/an-easy-way-of-creating-text-localization-and-detection-in-tesseract-ocr-24cc89ed6fbc\n",
    "def get_possible_title(res):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT\n",
    "    res: dictionary, result of pytesseract.image_to_data; should contain left, top. width, height, text\n",
    "    \"\"\"\n",
    "    max_h = 0\n",
    "    h_diff_allowance = 6 # margin of error for max_h, since same sizes get recognised as different sometimes\n",
    "    possible_title = []\n",
    "    prev_is_possible_title = False # whether prev is also a part of the possible title\n",
    "    hw_ratio = 1/2\n",
    "\n",
    "    for i, text in enumerate(res[\"text\"]):\n",
    "        h = res[\"height\"][i] # use this as font size\n",
    "        w = res[\"width\"][i]\n",
    "        prev = ()\n",
    "        if (text == \" \" or \"\") or len(text.split())==0:\n",
    "            #print('-')\n",
    "            continue\n",
    "        elif text == \"?\":\n",
    "            continue\n",
    "        elif h/w >= hw_ratio:\n",
    "            prev_is_possibel_title = False\n",
    "            continue\n",
    "        else:\n",
    "            #conf = int(res[\"conf\"][i]) # confidence level of result\n",
    "            #if conf > 60:\n",
    "            if abs(h-max_h)<=h_diff_allowance: # h in allowable range, \n",
    "                if prev_is_possible_title==True:\n",
    "                    possible_title.append({\"text\": text, \"index\": i, \"height\": h, \"width\": w})\n",
    "                else:\n",
    "                    possible_title = [{\"text\": text, \"index\": i, \"height\": h, \"width\": w}]\n",
    "                    prev_is_possible_title = True\n",
    "            elif h > max_h+h_diff_allowance:\n",
    "                print(\"new max found!\")\n",
    "                print(text)\n",
    "                # dump prev possible title, start new one\n",
    "                max_h = h\n",
    "                possible_title = [{\"text\": text, \"index\": i, \"height\": h, \"width\": w}] # store index and text\n",
    "                prev_is_possible_title=True # for the next iteration to check\n",
    "            else: # h < max_h-h_diff_allowance <- we're going from big text to small text\n",
    "                # keep possible title, go on looking at next word\n",
    "                # do nothing with max_h or possible_title\n",
    "                prev_is_possible_title=False\n",
    "                continue\n",
    "        print(\"possible title of this round:\" , possible_title)\n",
    "    return possible_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new max found!\n",
      "frontiers\n",
      "possible title of this round: [{'text': 'frontiers', 'index': 5, 'height': 25, 'width': 87}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}, {'text': 'Semantics,', 'index': 35, 'height': 26, 'width': 152}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}, {'text': 'Semantics,', 'index': 35, 'height': 26, 'width': 152}, {'text': 'Metaphors,', 'index': 36, 'height': 26, 'width': 155}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}, {'text': 'Semantics,', 'index': 35, 'height': 26, 'width': 152}, {'text': 'Metaphors,', 'index': 36, 'height': 26, 'width': 155}, {'text': 'and', 'index': 37, 'height': 21, 'width': 50}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}, {'text': 'Semantics,', 'index': 35, 'height': 26, 'width': 152}, {'text': 'Metaphors,', 'index': 36, 'height': 26, 'width': 155}, {'text': 'and', 'index': 37, 'height': 21, 'width': 50}, {'text': 'Schematological', 'index': 39, 'height': 27, 'width': 230}]\n",
      "possible title of this round: [{'text': 'How', 'index': 24, 'height': 21, 'width': 61}, {'text': 'Grammar', 'index': 25, 'height': 22, 'width': 130}, {'text': 'Introduces', 'index': 26, 'height': 21, 'width': 146}, {'text': 'Asymmetry', 'index': 28, 'height': 26, 'width': 157}, {'text': 'Into', 'index': 29, 'height': 21, 'width': 52}, {'text': 'Cognitive', 'index': 30, 'height': 27, 'width': 130}, {'text': 'Structures:', 'index': 32, 'height': 22, 'width': 151}, {'text': 'Compositional', 'index': 33, 'height': 26, 'width': 199}, {'text': 'Semantics,', 'index': 35, 'height': 26, 'width': 152}, {'text': 'Metaphors,', 'index': 36, 'height': 26, 'width': 155}, {'text': 'and', 'index': 37, 'height': 21, 'width': 50}, {'text': 'Schematological', 'index': 39, 'height': 27, 'width': 230}, {'text': 'Hybrids', 'index': 40, 'height': 26, 'width': 104}]\n"
     ]
    }
   ],
   "source": [
    "poss_title = get_possible_title(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 21, 61),\n",
       " ('Grammar', 22, 130),\n",
       " ('Introduces', 21, 146),\n",
       " ('Asymmetry', 26, 157),\n",
       " ('Into', 21, 52),\n",
       " ('Cognitive', 27, 130),\n",
       " ('Structures:', 22, 151),\n",
       " ('Compositional', 26, 199),\n",
       " ('Semantics,', 26, 152),\n",
       " ('Metaphors,', 26, 155),\n",
       " ('and', 21, 50),\n",
       " ('Schematological', 27, 230),\n",
       " ('Hybrids', 26, 104)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw rectangles around recognised words for human inspection\n",
    "\n",
    "for txt_obj in poss_title:\n",
    "    x = res[\"left\"][txt_obj[\"index\"]]\n",
    "    y = res[\"top\"][txt_obj[\"index\"]]\n",
    "    w = txt_obj[\"width\"]\n",
    "    h = txt_obj[\"height\"]\n",
    "\n",
    "    text = txt_obj[\"text\"]\n",
    "    conf = int(res[\"conf\"][txt_obj[\"index\"]])\n",
    "    if conf > 70:\n",
    "        text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5, (0, 0, 200), 2)\n",
    "im = Image.fromarray(image)\n",
    "im.save(\"bounded.jpg\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get paper info from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
